{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Market Data Only baseline\n",
    "<a href='https://www.kaggle.com/christofhenkel/market-data-nn-baseline'>Based On _</a>\n",
    "\n",
    "This is a fit of market data only (no news data used ) showing relatively good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76df99da946719334bb8f506b454f6b742474fb8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from kaggle.competitions import twosigmanews\n",
    "import time\n",
    "\n",
    "env = twosigmanews.make_env()\n",
    "(market_train, _) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7401ca00dddbb7f128ff53e0aca2f39e24eb48a3"
   },
   "outputs": [],
   "source": [
    "cat_cols = ['assetCode']\n",
    "num_cols = ['volume', 'close', 'open', \n",
    "            'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "            'returnsClosePrevMktres1','returnsOpenPrevMktres1',\n",
    "            'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "            'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n",
    "\n",
    "train_indices, val_indices = train_test_split(market_train.index.values,test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74e64b17fcbc7d6c8c84bbf38273a89ef49378ba"
   },
   "source": [
    "## Handling Numerical, Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c3cb4ae869f557d713838176b5d6d269440c33a"
   },
   "outputs": [],
   "source": [
    "## Handling categorical variables\n",
    "def encode(encoder, x):\n",
    "    len_encoder = len(encoder)\n",
    "    try:\n",
    "        id = encoder[x]\n",
    "    except KeyError:\n",
    "        id = len_encoder\n",
    "    return id\n",
    "\n",
    "encoders = [{} for cat in cat_cols]\n",
    "\n",
    "\n",
    "for i, cat in enumerate(cat_cols):\n",
    "    print('encoding %s ...' % cat, end=' ')\n",
    "    encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].astype(str).unique())}\n",
    "    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n",
    "    print('Done')\n",
    "\n",
    "embed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets\n",
    "\n",
    "## Handling numerical variables\n",
    "market_train[num_cols] = market_train[num_cols].fillna(0)\n",
    "print('scaling numerical columns')\n",
    "scaler = StandardScaler()\n",
    "market_train[num_cols] = scaler.fit_transform(market_train[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9857dee48ca862a8f2f11cdfa38983c1be57ce06"
   },
   "source": [
    "## Define NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0eb0d09deb700c9404dace38d15ec3abf8d8b4e"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.optimizers import SGD, Adam, Adagrad, RMSprop\n",
    "\n",
    "def build_model():\n",
    "    categorical_inputs = []\n",
    "    for cat in cat_cols:\n",
    "        categorical_inputs.append(Input(shape=[1], name=cat))\n",
    "    \n",
    "    categorical_embeddings = []\n",
    "    for i, cat in enumerate(cat_cols):\n",
    "        categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n",
    "        \n",
    "    #categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\n",
    "    categorical_logits = Flatten()(categorical_embeddings[0])\n",
    "    categorical_logits = Dense(32, activation = 'relu')(categorical_logits)\n",
    "    \n",
    "    numerical_inputs = Input(shape=(11,), name='num')\n",
    "    numerical_logits = BatchNormalization()(numerical_inputs)\n",
    "    \n",
    "    numerical_logits = Dense(128, activation='relu')(numerical_logits)\n",
    "    numerical_logits = Dense(64, activation='relu')(numerical_logits)\n",
    "    \n",
    "    logits = Concatenate()([numerical_logits, categorical_logits])\n",
    "    logits = Dense(64, activation='relu')(logits)\n",
    "    out = Dense(1, activation='sigmoid')(logits)\n",
    "    \n",
    "    model = Model(inputs=categorical_inputs + [numerical_inputs], outputs=out)\n",
    "    model.compile(optimizer=SGD(lr = 0.001), loss=binary_crossentropy)\n",
    "    return model\n",
    "\n",
    "build_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "771ffe90909a06a8904a6b3be0c7e79927fc2504"
   },
   "outputs": [],
   "source": [
    "def get_input(market_train, indices):\n",
    "    X_num = market_train.loc[indices, num_cols].values\n",
    "    X = {'num':X_num}\n",
    "    for cat in cat_cols:\n",
    "        X[cat] = market_train.loc[indices, cat_cols].values\n",
    "    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n",
    "    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n",
    "    u = market_train.loc[indices, 'universe']\n",
    "    d = market_train.loc[indices, 'time'].dt.date\n",
    "    return X,y,r,u,d\n",
    "\n",
    "# r, u and d are used to calculate the scoring metric\n",
    "X_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\n",
    "X_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d83f3932a1824f095a1cb39ed90e14ae375dcdfb"
   },
   "source": [
    "## Train NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9db79a200354262da83f480a0957139c6f9cf94",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "check_point = ModelCheckpoint('model.hdf5', verbose=True, save_best_only=True)\n",
    "early_stoping = EarlyStopping(patience=5, verbose=True)\n",
    "model =build_model()\n",
    "model.fit(X_train, y_train.astype(int),\n",
    "          validation_data = (X_valid, y_valid.astype(int)),\n",
    "          epochs = 25,\n",
    "          verbose = True,\n",
    "          callbacks = [early_stoping, check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22ef9ae6561d90cee2d3891e04ae97c2fd73e60d"
   },
   "source": [
    "## Evaluation of Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62b8672f092581c4790d437796a51e8fc004b03a"
   },
   "outputs": [],
   "source": [
    "# distribution of confidence that will be used as submission\n",
    "model.load_weights('model.hdf5')\n",
    "confidence_valid = model.predict(X_valid)[:,0]*2 -1\n",
    "print(accuracy_score(confidence_valid>0,y_valid))\n",
    "plt.hist(confidence_valid, bins='auto')\n",
    "plt.title(\"predicted confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cde94c41330af69f6a7c1da7360c1edb08d830ca"
   },
   "outputs": [],
   "source": [
    "# calculation of actual metric that is used to calculate final score\n",
    "r_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\n",
    "x_t_i = confidence_valid * r_valid * u_valid\n",
    "data = {'day' : d_valid, 'x_t_i' : x_t_i}\n",
    "df = pd.DataFrame(data)\n",
    "x_t = df.groupby('day').sum().values.flatten()\n",
    "mean = np.mean(x_t)\n",
    "std = np.std(x_t)\n",
    "score_valid = mean / std\n",
    "print(score_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d83d912fbf714db09bb1757605aca30adf6b4534"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4fa516fae1fc8c3d15cc6a636467c769e19b758"
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()\n",
    "\n",
    "n_days = 0\n",
    "prep_time = 0\n",
    "prediction_time = 0\n",
    "packaging_time = 0\n",
    "predicted_confidences = np.array([])\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    n_days +=1\n",
    "    print(n_days,end=' ')\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    market_obs_df['assetCode_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n",
    "\n",
    "    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n",
    "    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n",
    "    X_num_test = market_obs_df[num_cols].values\n",
    "    X_test = {'num':X_num_test}\n",
    "    X_test['assetCode'] = market_obs_df['assetCode_encoded'].values\n",
    "    \n",
    "    prep_time += time.time() - t\n",
    "    \n",
    "    t = time.time()\n",
    "    market_prediction = model.predict(X_test)[:,0]*2 -1\n",
    "    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n",
    "    prediction_time += time.time() -t\n",
    "    \n",
    "    t = time.time()\n",
    "    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n",
    "    # insert predictions to template\n",
    "    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n",
    "    env.predict(predictions_template_df)\n",
    "    packaging_time += time.time() - t\n",
    "\n",
    "env.write_submission_file()\n",
    "total = prep_time + prediction_time + packaging_time\n",
    "print(f'Preparing Data: {prep_time:.2f}s')\n",
    "print(f'Making Predictions: {prediction_time:.2f}s')\n",
    "print(f'Packing: {packaging_time:.2f}s')\n",
    "print(f'Total: {total:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d90bfaca8ecab7710bfc1dd40a4970fd58c5cb1c"
   },
   "outputs": [],
   "source": [
    "# distribution of confidence as a sanity check: they should be distributed as above\n",
    "plt.hist(predicted_confidences, bins='auto')\n",
    "plt.title(\"predicted confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d5c21d0b705d9193400608af25e6fac0a67ad65"
   },
   "outputs": [],
   "source": [
    "# Next: LSTM, correct train/ test split (making sure it is time ordered)\n",
    "# ... refere to original kernel (comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
