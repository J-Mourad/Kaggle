{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport xgboost as xgb\n\nfrom pprint import pprint\nimport math\n\nfrom scipy.stats import kurtosis, skew\n\nfrom IPython import embed\nfrom IPython.terminal.embed import InteractiveShellEmbed\n\nfrom sklearn.model_selection import KFold\n\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport shap\nplt.rcParams['figure.figsize'] = (12,6)\n\nimport os\nfrom os.path import join as pjoin\n\ndata_root = '../input/build-my-data'\nprint(os.listdir(data_root))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"085f4363ff7fd4919a273e2a10a8735b1c89cc97"},"cell_type":"code","source":"def load_data(data='train',n=2):\n    df = pd.DataFrame()\n    for i in range(n) :\n        if data=='train':\n            if i > 8 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'train_{i}.pkl'))\n        elif data=='test':\n            if i > 2 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'test_{i}.pkl'))\n        df = pd.concat([df,dfpart])\n        del dfpart\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8041378417cdcc34d4e2f9166cbafb66e3a12794"},"cell_type":"code","source":"%%time\ndf_train = load_data(n=9)\ndf_test = load_data('test',n=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8b21763cca9e1a9f6e7b3359ea181bea8a0c81b"},"cell_type":"code","source":"#df_train.isnull().sum()\n#df_test.isnull().sum()\ndf_all = pd.concat([df_train,df_test]).reset_index(drop=True)\nprint({\"all\":df_all.shape,\n       \"df_train\":df_train.shape,\n       \"df_test\":df_test.shape,})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac6383fda4975d53e69c22f477885b0aa0b27595"},"cell_type":"code","source":"def generate_label(label,id_dfx):\n    col_label=['fullVisitorId','totals_transactionRevenue']\n    #Select only the id is in df_train for the label\n    label = label[label.fullVisitorId.isin(id_dfx)].copy()\n    label=label.reset_index(drop=True)\n    #drop all columuns else fullvisitorsid and totaltransations\n    for c in label.columns:\n        if(c not in col_label ):\n            label.drop(c,axis=1,inplace=True)\n    #Select the id in train not in label       \n    id_label = label.fullVisitorId.drop_duplicates()\n    not_in_label=list(set(id_dfx) - set(id_label))\n    zeros=[0 for c in range(0,len(not_in_label))]\n    df_label_0=pd.DataFrame(list(zip(not_in_label, zeros)) ,columns=['fullVisitorId','totals_transactionRevenue'])\n    #Contatane te two dataframe\n    label=pd.concat([label,df_label_0]).reset_index(drop=True)\n    return label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9376f51bbc617bd5c2a645a8e18040a62645833"},"cell_type":"code","source":"from datetime import datetime\nfrom dateutil.relativedelta import relativedelta\ndef three_month_after(train_begin,train_end,label_begin,label_end,verbose=False):\n    tb=datetime.strptime(train_begin,'%Y-%m-%d')+relativedelta(months=3);\n    te=datetime.strptime(train_end,'%Y-%m-%d')+relativedelta(months=3);\n    lb=datetime.strptime(label_begin,'%Y-%m-%d')+relativedelta(months=3);\n    le=datetime.strptime(label_end,'%Y-%m-%d')+relativedelta(months=3);\n    if verbose:\n        print(\"train---------------------------------\")\n        print( 'intial:',datetime.strptime(train_begin,'%Y-%m-%d'))\n        print( 'After 3 Month:', tb.strftime('%Y-%m-%d'))\n        print( 'intial:',datetime.strptime(train_end,'%Y-%m-%d'))\n        print( 'After 3 Month:', te.strftime('%Y-%m-%d'))\n        print(\"label---------------------------------\")\n        print( 'intial:',datetime.strptime(label_begin,'%Y-%m-%d'))\n        print( 'After 3 Month:', lb.strftime('%Y-%m-%d'))\n        print( 'intial:',datetime.strptime(label_end,'%Y-%m-%d'))\n        print( 'After 3 Month:', le.strftime('%Y-%m-%d'))\n    df_x = df_all[ (df_all.date >= tb.strftime('%Y-%m-%d')) & (df_all.date <= te.strftime('%Y-%m-%d'))].copy() #5,5 months(oct ---> march *0.5)\n    df_x=df_x.reset_index(drop=True)\n    #<-> 1.5 month \n    label = df_all[(df_all.date >= lb.strftime('%Y-%m-%d')) & (df_all.date <= le.strftime('%Y-%m-%d'))].copy() #2 months (may--->june)\n    id_train = df_x.fullVisitorId.drop_duplicates()\n\n    #Generate label\n    label=generate_label(label,id_train).copy()\n    \n    return df_x,label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a06feee7c9f5c439edac2187036fa48820aa8e74"},"cell_type":"code","source":"# train_v2.csv - from August 1st 2016 to April 30th 2018.\n# test_v2.csv - from May 1st 2018 to October 15th 2018.\n# sample_submission_v2.csv - from December 1st 2018 to January 31st 2019\n\n\n#test\ndf_test_x = df_all[(df_all.date >= \"2018-05-01\") & (df_all.date <= \"2018-10-15\")].copy()\ndf_test_x =df_test_x.reset_index(drop=True)\n#df_test_y= \"sample submission\"\n\n#fold1\n#----------------------------train\ncol=['fullVisitorId','totals_transactionRevenue']\n#train1 \n#df_train1_x_agg.shape (377186, 67)\ndf_train1_x = df_all[df_all.date <= \"2016-12-30\"].copy() #5 months(august ---> dec)\ndf_train1_x=df_train1_x.reset_index(drop=True)\n#<-> 1.5 month (jan , fev *0.5)\nlabel_1 = df_all[(df_all.date >= \"2017-02-15\") & (df_all.date <= \"2017-04-15\")].copy() #2 months (feb*0.5--->april*0.5)\nid_train1 = df_train1_x.fullVisitorId.drop_duplicates()\n\n#Generate label\nlabel_1=generate_label(label_1,id_train1).copy()\n\n#items = set(not_in_label)\n#set(new_list).issubset(items)\n\n#len(id_train1)\n#len(label_1.fullVisitorId.drop_duplicates())\n\ndf_train1_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5554e66c741fea34f0d7996a7c8fbc70be28fb29"},"cell_type":"code","source":"df_train2_x,label_2=three_month_after(\"2016-08-01\",\"2016-12-30\",\"2017-02-15\",\"2017-04-15\",verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd59295611c4fde2b0fa0c9afd4b6c0f867aa423"},"cell_type":"code","source":"#train3 \ndf_train3_x,label_3=three_month_after(\"2016-11-01\",\"2017-03-30\",\"2017-05-15\",\"2017-07-15\",verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa5a783b7b51dbf002235446f235ef342fdd4094"},"cell_type":"code","source":"df_train4_x,label_4=three_month_after(\"2017-02-01\",\"2017-06-30\",\"2017-08-15\",\"2017-10-15\",verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34d7c6022256786bc3fa748c75cc99f757bfbc55"},"cell_type":"code","source":"#train5 \ndf_train5_x,label_5=three_month_after(\"2017-05-01\",\"2017-09-30\",\"2017-11-15\",\"2018-01-15\",verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"968e2a526bf3c46ce8ef9b70fc51723fdb77be12"},"cell_type":"code","source":"#train6\ndf_train6_x,label_6=three_month_after(\"2017-08-01\",\"2017-12-30\",\"2018-02-15\",\"2018-04-15\",verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ba277d9239e95eba85c7511e0029eaa24c9735e"},"cell_type":"code","source":"from pandas.io.json import json_normalize\ndef json_it(df,col):\n    for column in col:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        #df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n        df = df.merge(column_as_df, right_index=True, left_index=True).copy()\n    #print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    print(\"Done json it.... \")\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e37804bd786f1bf31398320b430ba184f61088e"},"cell_type":"code","source":"def group_by_transactionRevenue(df_x,y):\n    cat_cols=[\"channelGrouping\",\"device_browser\",\"device_deviceCategory\",\"device_operatingSystem\",\"geoNetwork_city\",\n          \"geoNetwork_continent\",\"geoNetwork_country\",\"geoNetwork_metro\", \"geoNetwork_networkDomain\",\n          \"geoNetwork_region\",\"geoNetwork_subContinent\", \"trafficSource_adContent\",\n          \"trafficSource_adwordsClickInfo.adNetworkType\",\"trafficSource_adwordsClickInfo.gclId\",\n          \"trafficSource_adwordsClickInfo.slot\",\"trafficSource_campaign\",\"trafficSource_keyword\",\n          \"trafficSource_medium\",\"trafficSource_referralPath\",\"trafficSource_source\",\"customDimensions_value\",\n          \"browser_category\",\"browser_operatingSystem\"]\n    \n    df_x_agg=df_x.groupby(\"fullVisitorId\",as_index=False)['totals_transactionRevenue'].sum().sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\n    y_agg = y.groupby(\"fullVisitorId\",as_index=False)['totals_transactionRevenue'].sum().sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\n    \n    # log totals_transactionRevenue\n    df_x_agg['totals_transactionRevenue'] = np.log1p(df_x_agg['totals_transactionRevenue'])\n    y_agg['totals_transactionRevenue'] = np.log1p(y_agg['totals_transactionRevenue'])\n    print(\"Done \")\n    #df_x_agg = pd.merge(df_x_agg_sum,df_x_agg_last, how='left',on=\"fullVisitorId\").sort_values(\"fullVisitorId\").reset_index(drop=True).copy()\n    return y_agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a0a7f3e45a85dcaf3d6cbf451b6791062374dac"},"cell_type":"code","source":"def timeSeries_2(df,col,agg):\n    print(\"Selected column: \",col)\n    if (agg==\"sum\"):\n        print(\"Selected agg: \",agg)\n        temp =df.groupby([\"fullVisitorId\", \"Date_Month\"],as_index=False)[col].sum().sort_values([\"fullVisitorId\",\"Date_Month\"]).reset_index(drop=True).copy()\n    elif (agg==\"count\"):\n        print(\"Selected agg: \",agg)\n        temp =df.groupby([\"fullVisitorId\", \"Date_Month\"],as_index=False)[col].count().sort_values([\"fullVisitorId\",\"Date_Month\"]).reset_index(drop=True).copy()\n    temp.reset_index(drop=True)\n    \n    if(col=='totals_transactionRevenue' and agg != \"count\"):\n        print(\"log applied....... \")\n        temp[col] = np.log1p(temp[col])\n        \n    print(\"Data shape : \",temp.shape)\n    #temp['totals_transactionRevenue'] = np.log1p(temp['totals_transactionRevenue'])\n    id_unique_df=list(df.fullVisitorId.drop_duplicates())\n    print(\"numbers of uniques fullVisitorId: \",len(id_unique_df))\n    months=list(df.Date_Month.drop_duplicates())\n    months.sort()\n    print(\"months  in df: \",months)\n    \n    nrows=temp.shape[0]\n    previous=\"??\"\n    dict_time_series={}\n    count=0\n    visitors_col=[]\n    agg_col=[]\n    for m in months:\n        dict_time_series[str(m)]=0.0\n    for index, row in temp.iterrows():\n        if previous != str(row[\"fullVisitorId\"]) and previous!=\"??\":\n            #print(previous,\" ---now id :-- \",row[\"fullVisitorId\"],\"   \",dict_time_series)\n            visitors_col.append(previous)\n            agg_col.append(dict_time_series.copy())\n            count=count+1\n            for m in months:\n                dict_time_series[str(m)]=0.0\n            previous=str(row[\"fullVisitorId\"])\n            dict_time_series[str(row[\"Date_Month\"])] = row[col]\n            if index == nrows-1:\n                print(\"the last: \")\n                print(previous,\" ----- \",dict_time_series)\n                visitors_col.append(previous)\n                agg_col.append(dict_time_series.copy())\n                count=count+1\n            \n        else:\n            #for i in range(visitor_agg.shape[0]):\n            #print(visitor_agg.iloc[i]['Date_Month'],visitor_agg.iloc[i][col])\n            dict_time_series[str(row[\"Date_Month\"])] = row[col]\n            previous=str(row[\"fullVisitorId\"])\n            if index == nrows-1:\n                print(\"the last: \")\n                print(previous,\" ----- \",dict_time_series)\n                visitors_col.append(previous)\n                agg_col.append(dict_time_series.copy())\n                count=count+1\n    print(\"index: \",index,\"counter\",count)\n    new_col_name=str(col)+\"_timeSeries_\"+str(agg)\n    d = {\"fullVisitorId\":visitors_col,new_col_name:agg_col}\n    df = pd.DataFrame(d)\n   #\n    df_j=json_it(df.copy(),[new_col_name])\n    \n    #\n    cols_timeS=[]\n    for m in months:\n        cols_timeS.append(new_col_name+'.'+str(m))\n    print(cols_timeS)\n    df_values=df_j[cols_timeS].values\n    print('get values done....')\n    return df_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"757aaa9e5ed76dd8b00cad6bca9d83eb585cff84"},"cell_type":"code","source":"print(\" -------------------------1---------------------------- \")\n#train1\ntimeS_trans_sum1=timeSeries_2(df_train1_x,'totals_transactionRevenue',\"sum\").copy()\ntimeS_trans_count1=timeSeries_2(df_train1_x,'totals_transactionRevenue',\"count\").copy()\ntimeS_bounce_sum1=timeSeries_2(df_train1_x,'totals_bounces',\"sum\").copy()\ntimeS_hit_sum1=timeSeries_2(df_train1_x,'totals_hits',\"sum\").copy()\ntimeS_pageV_sum1=timeSeries_2(df_train1_x,'totals_pageviews',\"sum\").copy()\nprint(\" --------------------------2---------------------------- \")\n#train2\ntimeS_trans_sum2=timeSeries_2(df_train2_x,'totals_transactionRevenue',\"sum\").copy()\ntimeS_trans_count2=timeSeries_2(df_train2_x,'totals_transactionRevenue',\"count\").copy()\ntimeS_bounce_sum2=timeSeries_2(df_train2_x,'totals_bounces',\"sum\").copy()\ntimeS_hit_sum2=timeSeries_2(df_train2_x,'totals_hits',\"sum\").copy()\ntimeS_pageV_sum2=timeSeries_2(df_train2_x,'totals_pageviews',\"sum\").copy()\nprint(\" ---------------------------3--------------------------- \")\n#train3\ntimeS_trans_sum3=timeSeries_2(df_train3_x,'totals_transactionRevenue',\"sum\").copy()\ntimeS_trans_count3=timeSeries_2(df_train3_x,'totals_transactionRevenue',\"count\").copy()\ntimeS_bounce_sum3=timeSeries_2(df_train3_x,'totals_bounces',\"sum\").copy()\ntimeS_hit_sum3=timeSeries_2(df_train3_x,'totals_hits',\"sum\").copy()\ntimeS_pageV_sum3=timeSeries_2(df_train3_x,'totals_pageviews',\"sum\").copy()\nprint(\" ----------------------------4--------------------------- \")\n#train4\ntimeS_trans_sum4=timeSeries_2(df_train4_x,'totals_transactionRevenue',\"sum\").copy()\ntimeS_trans_count4=timeSeries_2(df_train4_x,'totals_transactionRevenue',\"count\").copy()\ntimeS_bounce_sum4=timeSeries_2(df_train4_x,'totals_bounces',\"sum\").copy()\ntimeS_hit_sum4=timeSeries_2(df_train4_x,'totals_hits',\"sum\").copy()\ntimeS_pageV_sum4=timeSeries_2(df_train4_x,'totals_pageviews',\"sum\").copy()\nprint(\" -----------------------------5-------------------------- \")\n#train5\ntimeS_trans_sum5=timeSeries_2(df_train5_x,'totals_transactionRevenue',\"sum\").copy()\ntimeS_trans_count5=timeSeries_2(df_train5_x,'totals_transactionRevenue',\"count\").copy()\ntimeS_bounce_sum5=timeSeries_2(df_train5_x,'totals_bounces',\"sum\").copy()\ntimeS_hit_sum5=timeSeries_2(df_train5_x,'totals_hits',\"sum\").copy()\ntimeS_pageV_sum5=timeSeries_2(df_train5_x,'totals_pageviews',\"sum\").copy()\nprint(\" -----------------------------6-------------------------- \")\n#train6\ntimeS_trans_sum6=timeSeries_2(df_train6_x,'totals_transactionRevenue',\"sum\").copy()\ntimeS_trans_count6=timeSeries_2(df_train6_x,'totals_transactionRevenue',\"count\").copy()\ntimeS_bounce_sum6=timeSeries_2(df_train6_x,'totals_bounces',\"sum\").copy()\ntimeS_hit_sum6=timeSeries_2(df_train6_x,'totals_hits',\"sum\").copy()\ntimeS_pageV_sum6=timeSeries_2(df_train6_x,'totals_pageviews',\"sum\").copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b9ba7b813d9514826bf4421c464d09cd956d9e6"},"cell_type":"code","source":"# group by labels 2 months\nlabel_1=group_by_transactionRevenue(df_train1_x,label_1)\nlabel_2=group_by_transactionRevenue(df_train2_x,label_2)\nlabel_3=group_by_transactionRevenue(df_train3_x,label_3)\nlabel_4=group_by_transactionRevenue(df_train4_x,label_4)\nlabel_5=group_by_transactionRevenue(df_train5_x,label_5)\nlabel_6=group_by_transactionRevenue(df_train6_x,label_6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18927eeb3f4a67f1f4c45d310f244491847b4f54"},"cell_type":"code","source":"def rechape_train(list_timeS):\n    nb_features=5\n    first=list_timeS[0]\n    X_train_r = np.zeros((len(first), nb_features, len(list_timeS)))\n    i=0\n    for tS in list_timeS:\n        X_train_r[:, :,i ] = tS[:, :nb_features]\n        i=i+1\n    print(\"reshape done......\")\n    return X_train_r","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ea802cf272d925be31e0706928e155b6a4773f2"},"cell_type":"code","source":"# creating the rechape data x\n#train1\nX_train1_r=rechape_train([timeS_trans_sum1,timeS_trans_count1,timeS_bounce_sum1,timeS_hit_sum1,timeS_pageV_sum1])\n#train2\nX_train2_r=rechape_train([timeS_trans_sum2,timeS_trans_count2,timeS_bounce_sum2,timeS_hit_sum2,timeS_pageV_sum2])\n#train3\nX_train3_r=rechape_train([timeS_trans_sum3,timeS_trans_count3,timeS_bounce_sum3,timeS_hit_sum3,timeS_pageV_sum3])\n#train4\nX_train4_r=rechape_train([timeS_trans_sum4,timeS_trans_count4,timeS_bounce_sum4,timeS_hit_sum4,timeS_pageV_sum4])\n#train5\nX_train5_r=rechape_train([timeS_trans_sum5,timeS_trans_count5,timeS_bounce_sum5,timeS_hit_sum5,timeS_pageV_sum5])\n#train6\nX_train6_r=rechape_train([timeS_trans_sum6,timeS_trans_count6,timeS_bounce_sum6,timeS_hit_sum6,timeS_pageV_sum6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16cfd873664d8898b986553246c46efc141e74b1"},"cell_type":"code","source":"from keras import backend\ndef rmse(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(y_pred - y_true)))\n\nfrom keras.layers import Activation, Input, Dense, Embedding, concatenate, Flatten\nfrom keras.models import Model\nfrom keras.optimizers import SGD\nfrom keras.layers import Conv1D,Input,Dense,MaxPooling1D,Dropout,BatchNormalization,Activation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5ba647cc506d70f2b9e7192f1382288aa88135b"},"cell_type":"code","source":"def build_model(filter=32):\n    \"\"\"\"\"\n    input_layer_broswer = Input( shape=(1,) , name='broswer')\n    emb1 = Embedding( input_dim = 1+np.max( dc['broswer'] ), output_dim=3 )(input_layer_broswer)\n    emb1 = Flatten( )(emb1)\n    \n    input_layer_deviceCategory = Input( shape=(1,) , name='deviceCategory')\n    emb2 = Embedding( input_dim = 1+np.max( dc['deviceCategory'] ), output_dim=3 )(input_layer_deviceCategory)\n    emb2 = Flatten( )(emb2)\n    \n    \"\"\"\"\"\n    #block 1\n    input_layer_num = Input(shape=(5,5),name='timeS_sum')\n    conv1=Conv1D(filter,kernel_size=2,name='conv1',padding=\"same\")(input_layer_num)\n    bn1=BatchNormalization(epsilon=0.01,name='bn1')(conv1)\n    bn1=Activation('relu',name='bn_activation1')(bn1)\n    conv1_2=Conv1D(filter,kernel_size=2,name='conv1_2',padding=\"same\")(bn1)\n    act=Activation('relu',name='activation')(conv1_2)\n\n    #block2\n    conv2=Conv1D(filter*2,kernel_size=2,name='conv2',padding=\"same\")(act)\n    bn2=BatchNormalization(epsilon=0.01,name='bn2')(conv2)\n    bn2=Activation('relu',name='bn_activation2')(bn2)\n    conv2_1=Conv1D(filter*2,kernel_size=2,name='conv2_1',padding=\"same\")(bn2)\n    drop=Dropout(rate=0.3,name='drop')(conv2_1)\n    \n    drop = Flatten( )(drop)\n    dense1 = Dense(units=1000, name='dense1')(drop)\n    dense1 = Activation('relu', name='activation1')(dense1)\n    \n    output_layer = Dense(units=1)(dense1)\n    output_layer=Activation('sigmoid',name='activation2')(output_layer)\n    \n    model = Model([input_layer_num], output_layer)\n    \n    opt = SGD()\n    model.compile(optimizer=opt, loss=rmse, metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"089d04abc974b75494317d35f20207d22576d637"},"cell_type":"code","source":"model = build_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db4830ab1a193a7fc5698d55a0b4d0debcd01cfa"},"cell_type":"code","source":"valid_rmse={}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0996552bb874b3d00faccf63837344a7953ca127"},"cell_type":"code","source":"#fold1\nfold1=model.fit(x=X_train1_r, y=label_1[\"totals_transactionRevenue\"], batch_size=100, epochs=4, validation_data=[X_train2_r,label_2[\"totals_transactionRevenue\"]])\nvalid_rmse.update({'fold1': min(fold1.history[\"val_loss\"])}) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4efcbc1e0f1ba29650d3890a911d0cb4953f9e98"},"cell_type":"code","source":"#fold2\nmodel = build_model()\nfold2=model.fit(x=X_train2_r, y=label_2[\"totals_transactionRevenue\"], batch_size=100, epochs=4, validation_data=[X_train3_r,label_3[\"totals_transactionRevenue\"]])\nvalid_rmse.update({'fold2': min(fold2.history[\"val_loss\"])}) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07e90e27fbc07030da61a9da0ca726ee3fa51da5"},"cell_type":"code","source":"#fold3\nmodel = build_model()\nfold3=model.fit(x=X_train3_r, y=label_3[\"totals_transactionRevenue\"], batch_size=100, epochs=4, validation_data=[X_train4_r,label_4[\"totals_transactionRevenue\"]])\nvalid_rmse.update({'fold3': min(fold3.history[\"val_loss\"])}) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a91f9f41729d84f96321fa81e5b742dfa5580b0"},"cell_type":"code","source":"#fold4\nmodel = build_model()\nfold4=model.fit(x=X_train4_r, y=label_4[\"totals_transactionRevenue\"], batch_size=100, epochs=4, validation_data=[X_train5_r,label_5[\"totals_transactionRevenue\"]])\nvalid_rmse.update({'fold4': min(fold4.history[\"val_loss\"])}) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab660299fc9b61e5005dfb1bf879224181c7cac9"},"cell_type":"code","source":"#fold5\nmodel = build_model()\nfold5=model.fit(x=X_train5_r, y=label_5[\"totals_transactionRevenue\"], batch_size=100, epochs=4, validation_data=[X_train6_r,label_6[\"totals_transactionRevenue\"]])\nvalid_rmse.update({'fold5': min(fold5.history[\"val_loss\"])}) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cec6b8a267910fdfc36e319eb5f8ff7edcddb21"},"cell_type":"code","source":"#plot k fold logloss\nplt.plot(range(len(valid_rmse)), valid_rmse.values())\nplt.xticks(range(len(valid_rmse)), list(valid_rmse.keys()))\nplt.xticks(rotation=90)\n\nplt.title('model logloss')  \nplt.ylabel('logloss')  \nplt.xlabel('fold')  \nplt.legend(['valid']) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8ddbae992f3a4ca3ae3b4f44c89f8d47747695f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fab03ff26ccec2703fe40b5fddbf97bf84e00f4f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3791448f878ed2d3b69e8d2e3c75f043522c6a67"},"cell_type":"code","source":"#train2\ndf_train2_x,label_2=three_month_after(\"2016-08-01\",\"2017-01-15\",\"2017-03-01\",\"2017-04-30\",verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0c144fcd5c52abfe47abc49dbdf866a79db06cf"},"cell_type":"code","source":"# reshape train data\n#nb_features=5\n#X_train1_r = np.zeros((len(timeS_trans_sum1), nb_features, 2))\n#X_train1_r[:, :, 0] = timeS_trans_sum1[:, :nb_features]\n#X_train1_r[:, :, 1] = timeS_trans_count1[:, :nb_features]\n\n#X_train2_r = np.zeros((len(timeS_trans_sum2), nb_features, 2))\n#X_train2_r[:, :, 0] = timeS_trans_sum2[:, :nb_features]\n#X_train2_r[:, :, 1] = timeS_trans_count2[:, :nb_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f69a64eee27dba9960f8a42ecd55955da9927397"},"cell_type":"code","source":"def timeSeries(df,col,agg):\n    print(\"Selected column: \",col)\n    if (agg==\"sum\"):\n        print(\"Selected agg: \",agg)\n        temp =df.groupby([\"fullVisitorId\", \"Date_Month\"],as_index=False)[col].sum().sort_values([\"fullVisitorId\",\"Date_Month\"]).reset_index(drop=True).copy()\n    elif (agg==\"count\"):\n        print(\"Selected agg: \",agg)\n        temp =df.groupby([\"fullVisitorId\", \"Date_Month\"],as_index=False)[col].count().sort_values([\"fullVisitorId\",\"Date_Month\"]).reset_index(drop=True).copy()\n    temp.reset_index(drop=True)\n    #temp['totals_transactionRevenue'] = np.log1p(temp['totals_transactionRevenue'])\n    if(col=='totals_transactionRevenue' and agg != \"count\"):\n        print(\"log applied....... \")\n        temp[col] = np.log1p(temp[col])\n        \n    print(\"Data shape : \",temp.shape)\n    id_unique_df=list(df.fullVisitorId.drop_duplicates())\n    print(\"numbers of uniques fullVisitorId: \",len(id_unique_df))\n    months=list(df.Date_Month.drop_duplicates())\n    months.sort()\n    print(\"months  in df: \",months)\n    visitors_col=[]\n    agg_col=[]\n    for id_ in id_unique_df:\n        visitor_agg=temp[temp[\"fullVisitorId\"]== id_]\n        visitor_agg.reset_index(drop=True)\n        dict_time_series={}\n        for m in months:\n            dict_time_series[str(m)]=0.0\n        for i in range(visitor_agg.shape[0]):\n            #print(visitor_agg.iloc[i]['Date_Month'],visitor_agg.iloc[i][col])\n            dict_time_series[str(visitor_agg.iloc[i]['Date_Month'])] = visitor_agg.iloc[i][col]\n        #print(\"id: \",id_,\" time series : \",dict_time_series)\n        visitors_col.append(id_)\n        agg_col.append(dict_time_series)\n    d = {\"fullVisitorId\":visitors_col,str(col)+\"_timeSeries_\"+str(agg):agg_col}\n    df = pd.DataFrame(d)\n    print(\"done.....\")\n    return df\n#timeSeries(df_train1_x,'totals_transactionRevenue')\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}