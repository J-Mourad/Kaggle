{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import csv\nimport time\nfrom csv import DictReader\nfrom math import exp, sqrt\nfrom random import randint\n\n# TL; DR, the main training process starts on line: 250,\n# you may want to start reading the code from there\n\n\n##############################################################################\n# parameters #################################################################\n##############################################################################\n\n# A, paths\ndata_path = \"../input/\"\ntrain = data_path+'train.csv'               # path to training file\ntest = data_path+'test.csv'                 # path to testing file\n#synth = data_path+'synthetic-data-for-talkingdata-comp/synth.csv'\nsubmission = 'sub_proba.csv'  # path of to be outputted submission file\n\n\n##############################################################################\n# class, function, generator definitions #####################################\n##############################################################################\n\nclass ftrl_proximal(object):\n    ''' Our main algorithm: Follow the regularized leader - proximal\n\n        In short,\n        this is an adaptive-learning-rate sparse logistic-regression with\n        efficient L1-L2-regularization\n\n        Reference:\n        http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf\n    '''\n\n    def __init__(self, alpha, beta, L1, L2, D, sampling_factor):\n        # parameters\n        self.alpha = alpha\n        self.beta = beta\n        self.L1 = L1\n        self.L2 = L2\n\n        # feature related parameters\n        self.D = D\n        self.sf = sampling_factor\n\n        # model\n        # n: squared sum of past gradients\n        # z: weights\n        # w: lazy weights\n        self.n = [0.] * D\n        self.z = [0.] * D\n        self.w = {}\n\n    def _indices(self, x):\n        ''' A helper generator that yields the indices in x\n\n            The purpose of this generator is to make the following\n            code a bit cleaner when doing feature interaction.\n        '''\n\n        # first yield index of the bias term\n        yield 0\n\n        # then yield the normal indices\n        for index in x:\n            yield index\n     \n\n    def predict(self, x):\n        ''' Get probability estimation on x\n\n            INPUT:\n                x: features\n\n            OUTPUT:\n                probability of p(y = 1 | x; w)\n        '''\n\n        # parameters\n        alpha = self.alpha\n        beta = self.beta\n        L1 = self.L1\n        L2 = self.L2\n\n        # model\n        n = self.n\n        z = self.z\n        w = {}\n\n        # wTx is the inner product of w and x\n        wTx = 0.\n        for i in self._indices(x):\n            sign = -1. if z[i] < 0 else 1.  # get sign of z[i]\n\n            # build w on the fly using z and n, hence the name - lazy weights\n            # we are doing this at prediction instead of update time is because\n            # this allows us for not storing the complete w\n            if sign * z[i] <= L1:\n                # w[i] vanishes due to L1 regularization\n                w[i] = 0.\n            else:\n                # apply prediction time L1, L2 regularization to z and get w\n                w[i] = (sign * L1 - z[i]) / ((beta + sqrt(n[i])) / alpha + L2)\n\n            wTx += w[i]\n\n        # cache the current w for update stage\n        self.w = w\n\n        # bounded sigmoid function, this is the probability estimation\n        return 1. / (1. + exp(-max(min(wTx, 35.), -35.)))\n\n    def update(self, x, p, y):\n        ''' Update model using x, p, y\n\n            INPUT:\n                x: feature, a list of indices\n                p: click probability prediction of our model\n                y: answer\n\n            MODIFIES:\n                self.n: increase by squared gradient\n                self.z: weights\n        '''\n\n        # parameter\n        alpha = self.alpha\n\n        # model\n        n = self.n\n        z = self.z\n        w = self.w\n\n        # gradient under logloss\n        g = p - y\n        ############################################################################\n        ############################################################################\n        MAX_RANDOM_FACTOR = self.sf\n        MIN_RANDOM_FACTOR = self.sf - 100\n        \n        # if y == 0, the sampling factor f should be 1, else a random integer between the two\n        # limits\n        f = 1 if y == 0 else randint(MIN_RANDOM_FACTOR, MAX_RANDOM_FACTOR)\n        ############################################################################\n        ############################################################################\n\n\n        # update z and n\n        for i in self._indices(x):\n            sigma = (sqrt(n[i] + g * g) - sqrt(n[i])) / alpha\n            z[i] += f*(g - sigma * w[i])\n            n[i] += f*(g * g)\n\n\ndef data(path, D):\n    ''' GENERATOR: Apply hash-trick to the original csv row\n                   and for simplicity, we one-hot-encode everything\n\n        INPUT:\n            path: path to training or testing file\n            D: the max index that we can hash to\n\n        YIELDS:\n            ID: id of the instance, mainly useless\n            x: a list of hashed and one-hot-encoded 'indices'\n               we only need the index since all values are either 0 or 1\n            y: y = 1 if we have a click, else we have y = 0\n    '''\n    for t, row in enumerate(DictReader(open(path))):\n        x = []\n        y = 0.\n        \n        # Parse hour and date\n        date, tim = row['click_time'].split(' ')\n        chour = tim.split(':')[0]\n        x.append(abs(hash('hour_%s'%(chour))) % D)\n        \n        # process clicks        \n        if 'is_attributed' in row:\n            if row['is_attributed'] == '1':\n                y = 1.\n                #date, tim = row['attributed_time'].split(' ')\n                #ahour = tim.split(':')[0]\n            del row['is_attributed'], row['attributed_time']\n            #x.append(abs(hash('chour_%s___%s_ahour'%(chour, ahour))) % D)\n            \n        try:\n            click_id = row['click_id']\n        except:\n            click_id = ''\n            \n        \n        # Add the rest of the features\n        for k, v in row.items():\n            x.append(abs(hash('%s_%s'%(k, v))) % D)\n        \n        # Add an interaction\n        x.append(abs(hash('%s_os__chl_%s'%(row['channel'], row['os']))) % D)\n        x.append(abs(hash('%s_device_ip_%s'%(row['device'], row['ip']))) % D)\n        x.append(abs(hash('%s_app_chl_%s'%(row['app'], row['channel']))) % D)\n        x.append(abs(hash('%s_app_device_%s'%(row['app'], row['device']))) % D)\n        x.append(abs(hash('%s_device_chl_%s'%(row['device'], row['channel']))) % D)\n        x.append(abs(hash('%s_ip_chl_%s'%(row['ip'], row['channel']))) % D)\n        x.append(abs(hash('%s_ip_os_%s'%(row['ip'], row['os']))) % D)\n        x.append(abs(hash('%s_app_os_%s'%(row['os'], row['app']))) % D)\n        x.append(abs(hash('%s_ip_%s_app__device_%s'%(row['ip'],row['app'], row['device']))) % D)\n        \n        yield t, x, y, date, click_id\n\n\n\n##############################################################################\n# start training #############################################################\n##############################################################################\n# B, model\nalpha = 0.0146 # learning rate\nbeta = 0.00011   # smoothing parameter for adaptive learning rate\nL1 = 0.00011    # L1 regularization, larger value means more regularized\nL2 = 0.01    # L2 regularization, larger value means more regularized\nsampling_factor = 400 # this value is used to multiply entries of y=1\n                      # rows, in order to increase their weight\n# C, feature/hash trick\nD = 2 ** 26             # number of weights to use\n\n# initialize ourselves a learner\nlearner = ftrl_proximal(alpha, beta, L1, L2, D, sampling_factor)\nstart_time = time.time()\n\n\n# start training\nfor t, x, y, date, _ in data(train, D):  # data is a generator\n    p = learner.predict(x)\n    learner.update(x, p, y)\n    if t%1000000 == 0 and t != 0:\n        print(\"Train Rows Processed: %sM ; %ss \"%( int(t/1e+6), '%0.0f'%(time.time()-start_time)))\n\nprint(\"Train time: %f\" %(int(time.time()-start_time)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"##############################################################################\n# start testing, and build Kaggle's submission file ##########################\n##############################################################################\n\nstart_time = time.time()\n\nwith open(submission, 'w') as outfile:\n    outfile.write('click_id,is_attributed\\n')\n    for t, x, y, date, click_id in data(test, D):\n        p = learner.predict(x)\n        outfile.write('%s,%s\\n' % (click_id, str(p)))\n        if t%1000000 == 0:\n            print(\"Test Rows Processed: %sM ; %ss \"%( int(t/1e+6), '%0.0f'%(time.time()-start_time)))\n            \nprint(\"Test time: %f\" %(int(time.time()-start_time)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}